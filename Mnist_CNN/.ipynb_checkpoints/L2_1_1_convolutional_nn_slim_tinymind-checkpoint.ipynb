{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在TinyMind上运行代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import is over\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# tensorflow 工具包\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "# 打印运行时间\n",
    "import time\n",
    "# 导入tinymind环境自带的变量\n",
    "try: \n",
    "    from tinyenv.flags import flags\n",
    "except ImportError:\n",
    "    # 若在本地运行，则自动生成相同的class\n",
    "    class flags(object):\n",
    "        def __init__(self):\n",
    "            self.iterations=10000\n",
    "            self.learning_rate = 0.005\n",
    "            self.batch_size = 64\n",
    "            self.dropout = 0.5\n",
    "            self.decay = 0.95\n",
    "            self.output_dir = './output'\n",
    "            self.data_dir = './tmp/mnist'\n",
    "            self.f1_ck=[6,6]\n",
    "            self.f1_cc=32\n",
    "            self.f2_ck=[6,6]\n",
    "            self.f2_cc=64\n",
    "            self.features=1024\n",
    "            self.l_r_s=4e-3\n",
    "            self.l_r_e = 1e-4            \n",
    "            self.l2_regularizer=7e-5\n",
    "#实例化class\n",
    "FLAGS = flags()\n",
    "\n",
    "print('import is over')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义主要运行函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class is over\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Mnist_CNN(object):\n",
    "    \"\"\"mnist的CNN实现，默认参数test中正确率为99%左右\n",
    "    \n",
    "    第一层: 卷积核为f1_ck=[6,6], 1通道->f1_cc=32通道; \n",
    "    最大池化, 核为f1_pc=[2,2], 步长为f1_ps=2\n",
    "    第二层: 卷积核为f2_ck=[6,6], 32通道->f2_cc=64通道; \n",
    "    最大池化, 核为f2_pk=[2,2], 步长为f2_ps=[2,2]\n",
    "    先平均池化, 核为全部数据; \n",
    "    再卷积，核为1, 输出通道为features=1024的向量\n",
    "    dropout概率为drop=0.5之后, 卷积为10的向量; 经softmax激活\n",
    "    损失函数为：交叉熵+l2_regularizer=7e-5*L2损失\n",
    "    利用Adam方法, 初始学习率为l_r_s=4e-3, 循环for_=6000次\n",
    "    最终训练结束时学习率为l_r_e=1e-4， 每次batch=100, \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,f1_ck=[6,6],f1_cc=32,f1_pc=[2,2],f1_ps=2,\n",
    "                 f2_ck=[6,6],f2_cc=64,f2_pk=[2,2],f2_ps=2,\n",
    "                 features=1024, drop=0.5, l2_regularizer=7e-5,\n",
    "                 l_r_s=4e-3, l_r_e = 1e-4, for_=10000,batch=64):\n",
    "        #学习率衰减\n",
    "        l_r_decay = (l_r_s - l_r_e)/for_\n",
    "        lr=l_r_s\n",
    "        # 导入数据\n",
    "        mnist = input_data.read_data_sets(FLAGS.data_dir+'/mnist',  one_hot=True,)\n",
    "        \n",
    "        #创建新的空白运行图\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.InteractiveSession()\n",
    "        \n",
    "        # 输入数据维度\n",
    "        with tf.name_scope('input'):\n",
    "            x = tf.placeholder(tf.float32, [None, 784])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "        \n",
    "        # 将x转为4阶张量\n",
    "        with tf.name_scope('reshape'):\n",
    "            x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        \n",
    "        # 第一层之卷积,卷积核为[6,6]\n",
    "        # [-1, 28, 28, 1] => [-1, 28, 28, 32]\n",
    "        with tf.name_scope('conv1'):\n",
    "            h_conv1 = tf.contrib.slim.conv2d(x_image, f1_cc, f1_ck, \n",
    "                                             padding='SAME', \n",
    "                                             activation_fn=tf.nn.tanh)\n",
    "        tf.summary.scalar('l2_regularizer_f1', tf.contrib.layers.l2_regularizer(0.1)(h_conv1))\n",
    "        \n",
    "        # 第一层之最大池化, 核[2,2], 步长2\n",
    "        # [-1, 28, 28, 32] => [-1, 14, 14, 32]\n",
    "        with tf.name_scope('pool1'):\n",
    "            h_pool1 = tf.contrib.slim.max_pool2d(h_conv1, f1_pc, stride=f1_ps, \n",
    "                                                 padding='VALID')  \n",
    "        # 第二层之卷积,卷积核为[6,6]\n",
    "        # [-1, 14, 14, 32] => [-1, 14, 14, 64]\n",
    "        with tf.name_scope('conv2'):\n",
    "            h_conv2 = tf.contrib.slim.conv2d(h_pool1, f2_cc, f2_ck, \n",
    "                                             padding='SAME', \n",
    "                                             activation_fn=tf.nn.relu)\n",
    "        \n",
    "        # 第二层之最大池化, 核[2,2], 步长2\n",
    "        # [-1, 14, 14, 64] => [-1, 7, 7, 64]\n",
    "        with tf.name_scope('pool2'):\n",
    "            h_pool2 = tf.contrib.slim.max_pool2d(h_conv2, f2_pk, stride=f2_ps, \n",
    "                                                 padding='VALID')\n",
    "        \n",
    "        \n",
    "        # 第三层之先池化再卷积,\n",
    "        # 池化核[7,7]，步长1，[-1, 7, 7, 64] => [-1, 1, 1, 64]\n",
    "        # 卷积核[1,1]，[-1, 1, 1, 64] => [-1, 1, 1, 1024]\n",
    "        with tf.name_scope('fc1'):\n",
    "            h_pool2_flat = tf.contrib.slim.avg_pool2d(h_pool2, \n",
    "                                                      h_pool2.shape[1:3], \n",
    "                                                      stride=[1, 1], \n",
    "                                                      padding='VALID')\n",
    "            h_fc1 = tf.contrib.slim.conv2d(h_pool2_flat, features, [1, 1], \n",
    "                                           activation_fn=tf.nn.relu)\n",
    "        \n",
    "        # 避免过拟合，加入Dropout\n",
    "        with tf.name_scope('fc1_dropout'):\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        tf.summary.histogram('fc1_dropout', h_fc1_drop)\n",
    "        tf.summary.scalar('fc1_dropout_l2_regularizer', tf.contrib.layers.l2_regularizer(0.1)(h_fc1_drop))\n",
    "        \n",
    "        # 卷积核[1,1]，[-1, 1, 1, 1024] => [-1, 1, 1, 10]\n",
    "        with tf.name_scope('fc2'):\n",
    "            y = tf.squeeze(tf.contrib.slim.conv2d(h_fc1_drop, 10, [1,1], \n",
    "                                                  activation_fn=None))\n",
    "        # 定义损失函数\n",
    "        with tf.name_scope('loss'):\n",
    "            # 交叉熵损失\n",
    "            cross_entropy = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "            # 所有VARIABLES的l2损失\n",
    "            l2_loss = tf.add_n( [tf.nn.l2_loss(w) for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)] )\n",
    "            total_loss = cross_entropy + l2_regularizer*l2_loss\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "        tf.summary.scalar('l2_loss', l2_loss)\n",
    "        tf.summary.scalar('total_loss', total_loss)\n",
    "        \n",
    "        # 定义评价函数，正确率\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "            \n",
    "        # 优化方法\n",
    "        with tf.name_scope('train_step'):\n",
    "            #global_step = tf.Variable(0)  \n",
    "            #learning_rate = tf.train.exponential_decay(0.1, global_step, 100, 0.96, staircase=True)  \n",
    "            learning_rate = tf.placeholder(tf.float32)\n",
    "            #train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "            #train_step = tf.train.MomentumOptimizer(learning_rate).minimize(total_loss)\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "        \n",
    "        # 全部变量初始化\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # 构建summary\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        train_summary_writer = tf.summary.FileWriter(FLAGS.output_dir+'/train/', \n",
    "                                                     graph = sess.graph,)\n",
    "                                                     #filename_suffix='train_{}'.format(***))\n",
    "        test_summary_writer = tf.summary.FileWriter(FLAGS.output_dir+'/test/', )\n",
    "                                                    #filename_suffix='test_{}'.format(***))\n",
    "        \n",
    "        print('Session is over')\n",
    "        \n",
    "        # Train\n",
    "        start = time.time()\n",
    "        for step in range(for_):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch)\n",
    "            lr -= l_r_decay\n",
    "            _, total_loss_value = sess.run([train_step, total_loss], \n",
    "                                           feed_dict={x: batch_xs, \n",
    "                                                      y_: batch_ys, \n",
    "                                                      learning_rate:lr, \n",
    "                                                      keep_prob:drop})\n",
    "\n",
    "\n",
    "            if (step+1)%100==0:\n",
    "                summary_test,acc = sess.run([summary_op,accuracy],\n",
    "                                           feed_dict={x: mnist.test.images,\n",
    "                                                      y_: mnist.test.labels,\n",
    "                                                      learning_rate:lr,\n",
    "                                                      keep_prob:1.0})\n",
    "                test_summary_writer.add_summary(summary_test, step+1)\n",
    "                \n",
    "                summary_train,acc_train = sess.run([summary_op,accuracy],\n",
    "                                           feed_dict={x: batch_xs, \n",
    "                                          y_: batch_ys, \n",
    "                                          learning_rate:lr, \n",
    "                                          keep_prob:drop})\n",
    "                train_summary_writer.add_summary(summary_train, step+1)\n",
    "            if (step+1)%500==0:\n",
    "                print('accuracy of test at step {0}:{1}, time is {2}'.format(step+1, \n",
    "                                                    acc, int(time.time()-start)))\n",
    "\n",
    "        sess.close()        \n",
    "        print('teain is over, totle time', int(time.time()-start))\n",
    "\n",
    "print('class is over')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sess = tf.Session()\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "lr=float(5/3)\n",
    "print(sess.run(learning_rate,feed_dict={learning_rate:lr}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./tmp/mnist/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./tmp/mnist/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./tmp/mnist/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./tmp/mnist/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Session is over\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9273b1f3b1a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMnist_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfor_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-c8733d2f2d86>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f1_ck, f1_cc, f1_pc, f1_ps, f2_ck, f2_cc, f2_pk, f2_ps, features, drop, l2_regularizer, l_r_s, l_r_e, for_, batch)\u001b[0m\n\u001b[1;32m    137\u001b[0m                                                       \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                                                       \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                                                       keep_prob:drop})\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhou/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhou/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhou/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhou/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhou/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mni = Mnist_CNN(for_=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T12:07:46.249413Z",
     "start_time": "2018-04-12T12:07:37.674520Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./tmp/mnist/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./tmp/mnist/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./tmp/mnist/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./tmp/mnist/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Session is over\n",
      "accuracy of batch at step 500:0.9599999785423279, time is 129\n",
      "accuracy of batch at step 1000:0.9599999785423279, time is 262\n",
      "accuracy of batch at step 1500:1.0, time is 392\n",
      "accuracy of batch at step 2000:0.9900000095367432, time is 520\n",
      "accuracy of batch at step 2500:0.9800000190734863, time is 650\n",
      "accuracy of batch at step 3000:0.9700000286102295, time is 780\n",
      "accuracy of batch at step 3500:1.0, time is 916\n",
      "accuracy of batch at step 4000:0.9900000095367432, time is 1051\n",
      "accuracy of batch at step 4500:0.9900000095367432, time is 1181\n",
      "accuracy of batch at step 5000:1.0, time is 1322\n",
      "teain is over, totle time 1322\n"
     ]
    }
   ],
   "source": [
    "mni = Mnist_CNN(f1_ck=FLAGS.f1_ck, f1_cc=FLAGS.f1_cc, f2_ck=FLAGS.f2_ck, \n",
    "                f2_cc=FLAGS.f2_cc, features=FLAGS.features, drop=FLAGS.dropout, \n",
    "                l2_regularizer= FLAGS.l2_regularizer, l_r_s=FLAGS.l_r_s, l_r_e = FLAGS.l_r_e, \n",
    "                for_=1000, batch=FLAGS.batch_size)\n",
    "#FLAGS.iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
