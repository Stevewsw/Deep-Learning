{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在TinyMind上运行代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import is over\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# tensorflow 工具包\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "# 打印运行时间\n",
    "import time\n",
    "# 导入tinymind环境自带的变量\n",
    "try: \n",
    "    from tinyenv.flags import flags\n",
    "except ImportError:\n",
    "    # 若在本地运行，则自动生成相同的class\n",
    "    class flags(object):\n",
    "        def __init__(self):\n",
    "            self.iterations=20000\n",
    "            self.learning_rate = 0.005\n",
    "            self.batch_size = 64\n",
    "            self.dropout = 0.5\n",
    "            self.decay = 0.95\n",
    "            self.output_dir = './output'\n",
    "            self.data_dir = './tmp/mnist'\n",
    "            self.f1_ck=[6,6]\n",
    "            self.f1_cc=32\n",
    "            self.f2_ck=[6,6]\n",
    "            self.f2_cc=64\n",
    "            self.features=1024\n",
    "            self.l_r_s=4e-3\n",
    "            self.l_r_e = 1e-4            \n",
    "            self.l2_regularizer=7e-5\n",
    "#实例化class\n",
    "FLAGS = flags()\n",
    "\n",
    "print('import is over')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义主要运行函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class is over\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Mnist_CNN(object):\n",
    "    \"\"\"mnist的CNN实现，默认参数test中正确率为99%左右\n",
    "    \n",
    "    第一层: 卷积核为f1_ck=[6,6], 1通道->f1_cc=32通道; (1152)\n",
    "    最大池化, 核为f1_pc=[2,2], 步长为f1_ps=2\n",
    "    第二层: 卷积核为f2_ck=[6,6], 32通道->f2_cc=64通道; (2304)\n",
    "    最大池化, 核为f2_pk=[2,2], 步长为f2_ps=[2,2]\n",
    "    先平均池化, 核为全部数据; \n",
    "    再卷积，核为1, 输出通道为features=1024的向量 (1024)\n",
    "    dropout概率为drop=0.5之后, 卷积为10的向量;(10) 经softmax激活\n",
    "    损失函数为：交叉熵+l2_regularizer=7e-5*L2损失\n",
    "    利用Adam方法, 初始学习率为l_r_s=4e-3, 循环for_=6000次\n",
    "    最终训练结束时学习率为l_r_e=1e-4， 每次batch=64,\n",
    "    未知数共(4490)个\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,f1_ck=[6,6],f1_cc=32,f1_pc=[2,2],f1_ps=2,\n",
    "                 f2_ck=[6,6],f2_cc=64,f2_pk=[2,2],f2_ps=2,\n",
    "                 features=1024, drop=0.5, l2_regularizer=7e-5,\n",
    "                 lr_start=4e-3, lr_end = 1e-4, maxstep=20000, batch=64):\n",
    "        #学习率衰减y=k/(step+b)\n",
    "        b=(maxstep)*lr_end/(lr_start-lr_end) \n",
    "        k=b*lr_start\n",
    "        \n",
    "        # 导入数据\n",
    "        mnist = input_data.read_data_sets(FLAGS.data_dir+'/mnist', one_hot=True,)\n",
    "        \n",
    "        #创建新的空白运行图\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.InteractiveSession()\n",
    "        \n",
    "        # 输入数据维度\n",
    "        with tf.name_scope('input'):\n",
    "            x = tf.placeholder(tf.float32, [None, 784])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "        \n",
    "        # 将x转为4阶张量\n",
    "        with tf.name_scope('reshape'):\n",
    "            x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        \n",
    "        # 第一层之卷积,卷积核为[6,6]\n",
    "        # [-1, 28, 28, 1] => [-1, 28, 28, 32]\n",
    "        with tf.name_scope('conv1'):\n",
    "            h_conv1 = tf.contrib.slim.conv2d(x_image, f1_cc, f1_ck, \n",
    "                                             padding='SAME', \n",
    "                                             activation_fn=tf.nn.tanh,\n",
    "                                             weights_initializer=tf.random_normal_initializer(0.0, 0.1),)\n",
    "        #tf.summary.scalar('conv1_l2_regularizer', tf.contrib.layers.l2_regularizer(0.1)(h_conv1))\n",
    "        \n",
    "        # 第一层之最大池化, 核[2,2], 步长2\n",
    "        # [-1, 28, 28, 32] => [-1, 14, 14, 32]\n",
    "        with tf.name_scope('pool1'):\n",
    "            h_pool1 = tf.contrib.slim.max_pool2d(h_conv1, f1_pc, stride=f1_ps, \n",
    "                                                 padding='VALID')  \n",
    "        # 第二层之卷积,卷积核为[6,6]\n",
    "        # [-1, 14, 14, 32] => [-1, 14, 14, 64]\n",
    "        with tf.name_scope('conv2'):\n",
    "            h_conv2 = tf.contrib.slim.conv2d(h_pool1, f2_cc, f2_ck, \n",
    "                                             padding='SAME', \n",
    "                                             activation_fn=tf.nn.relu,\n",
    "                                             weights_initializer=tf.truncated_normal_initializer(0.0, 0.1),)\n",
    "        \n",
    "        # 第二层之最大池化, 核[2,2], 步长2\n",
    "        # [-1, 14, 14, 64] => [-1, 7, 7, 64]\n",
    "        with tf.name_scope('pool2'):\n",
    "            h_pool2 = tf.contrib.slim.max_pool2d(h_conv2, f2_pk, stride=f2_ps, \n",
    "                                                 padding='VALID')\n",
    "        \n",
    "        \n",
    "        # 第三层之先池化再卷积,\n",
    "        # 池化核[7,7]，步长1，[-1, 7, 7, 64] => [-1, 1, 1, 64]\n",
    "        # 卷积核[1,1]，[-1, 1, 1, 64] => [-1, 1, 1, 1024]\n",
    "        with tf.name_scope('fc1'):\n",
    "            h_pool2_flat = tf.contrib.slim.avg_pool2d(h_pool2, \n",
    "                                                      h_pool2.shape[1:3], \n",
    "                                                      stride=[1, 1], \n",
    "                                                      padding='VALID')\n",
    "            h_fc1 = tf.contrib.slim.conv2d(h_pool2_flat, features, [1, 1], \n",
    "                                           activation_fn=tf.nn.relu,\n",
    "                                           weights_initializer=tf.truncated_normal_initializer(0.0, 0.1),)\n",
    "        \n",
    "        # 避免过拟合，加入Dropout\n",
    "        with tf.name_scope('fc1_dropout'):\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        tf.summary.histogram('fc1_dropout', h_fc1_drop)\n",
    "        tf.summary.scalar('fc1_dropout_l2_regularizer', tf.contrib.layers.l2_regularizer(0.1)(h_fc1_drop))\n",
    "        \n",
    "        # 卷积核[1,1]，[-1, 1, 1, 1024] => [-1, 1, 1, 10]\n",
    "        with tf.name_scope('fc2'):\n",
    "            y = tf.squeeze(tf.contrib.slim.conv2d(h_fc1_drop, 10, [1,1], \n",
    "                                                  activation_fn=None))\n",
    "        # 定义损失函数\n",
    "        with tf.name_scope('loss'):\n",
    "            # 交叉熵损失\n",
    "            cross_entropy = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "            # 所有VARIABLES的l2损失\n",
    "            l2_loss = tf.add_n( [tf.nn.l2_loss(w) for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)] )\n",
    "            total_loss = cross_entropy + l2_regularizer*l2_loss\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "        tf.summary.scalar('l2_loss', l2_loss)\n",
    "        tf.summary.scalar('total_loss', total_loss)\n",
    "        \n",
    "        # 定义评价函数，正确率\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "            \n",
    "        # 优化方法\n",
    "        with tf.name_scope('train_step'):\n",
    "            #global_step = tf.Variable(0)  \n",
    "            #learning_rate = tf.train.exponential_decay(0.1, global_step, 100, 0.96, staircase=True)  \n",
    "            learning_rate = tf.placeholder(tf.float32)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "        \n",
    "        # 全部变量初始化\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # 构建summary\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        #train_summary_writer = tf.summary.FileWriter(FLAGS.output_dir+'/train/', \n",
    "                                                     #graph = sess.graph,)\n",
    "                                                     #filename_suffix='train_{}'.format(***))\n",
    "\n",
    "        test_summary_writer = tf.summary.FileWriter(FLAGS.output_dir+'/test/', \n",
    "                                                    graph = sess.graph,)\n",
    "                                                    #filename_suffix='test_{}'.format(***))\n",
    "        \n",
    "        print('Session is over')\n",
    "        \n",
    "        # Train\n",
    "        lr=lr_start\n",
    "        start = time.time()\n",
    "        for step in range(maxstep):\n",
    "            if (step+1)%500==0: lr = k/(step+b)\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch)\n",
    "            _, total_loss_value = sess.run([train_step, total_loss], \n",
    "                                           feed_dict={x: batch_xs, \n",
    "                                                      y_: batch_ys, \n",
    "                                                      learning_rate:lr, \n",
    "                                                      keep_prob:drop})\n",
    "\n",
    "\n",
    "            if (step+1)%100==0:\n",
    "                summary_test,acc = sess.run([summary_op,accuracy],\n",
    "                                           feed_dict={x: mnist.test.images,\n",
    "                                                      y_: mnist.test.labels,\n",
    "                                                      learning_rate:lr,\n",
    "                                                      keep_prob:1.0})\n",
    "                test_summary_writer.add_summary(summary_test, step+1)\n",
    "                \n",
    "            \n",
    "            if (step+1)%500==0:\n",
    "                print('accuracy of test at step {0}:{1}, time is {2}'.format(step+1, \n",
    "                                                    acc, int(time.time()-start)))\n",
    "\n",
    "        sess.close()        \n",
    "        print('teain is over, totle time', int(time.time()-start))\n",
    "\n",
    "print('class is over')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实际运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T12:07:46.249413Z",
     "start_time": "2018-04-12T12:07:37.674520Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./tmp/mnist/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./tmp/mnist/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./tmp/mnist/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./tmp/mnist/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Session is over\n",
      "accuracy of test at step 500:0.9609000086784363, time is 91\n",
      "accuracy of test at step 1000:0.9708999991416931, time is 182\n",
      "accuracy of test at step 1500:0.9824000000953674, time is 277\n",
      "accuracy of test at step 2000:0.9843999743461609, time is 370\n",
      "accuracy of test at step 2500:0.9848999977111816, time is 463\n",
      "accuracy of test at step 3000:0.9864000082015991, time is 555\n",
      "accuracy of test at step 3500:0.9857000112533569, time is 647\n",
      "accuracy of test at step 4000:0.986299991607666, time is 741\n",
      "accuracy of test at step 4500:0.9840999841690063, time is 837\n",
      "accuracy of test at step 5000:0.9847999811172485, time is 935\n",
      "accuracy of test at step 5500:0.9879999756813049, time is 1034\n",
      "accuracy of test at step 6000:0.9878000020980835, time is 1128\n",
      "accuracy of test at step 6500:0.9883999824523926, time is 1221\n",
      "accuracy of test at step 7000:0.9887999892234802, time is 1312\n",
      "accuracy of test at step 7500:0.9890999794006348, time is 1406\n",
      "accuracy of test at step 8000:0.9878000020980835, time is 1498\n",
      "accuracy of test at step 8500:0.9882000088691711, time is 1591\n",
      "accuracy of test at step 9000:0.9890999794006348, time is 1684\n",
      "accuracy of test at step 9500:0.989799976348877, time is 1776\n",
      "accuracy of test at step 10000:0.9894000291824341, time is 1871\n",
      "accuracy of test at step 10500:0.9891999959945679, time is 1964\n",
      "accuracy of test at step 11000:0.9896000027656555, time is 2055\n",
      "accuracy of test at step 11500:0.9891999959945679, time is 2146\n",
      "accuracy of test at step 12000:0.989799976348877, time is 2238\n",
      "accuracy of test at step 12500:0.9896000027656555, time is 2332\n",
      "accuracy of test at step 13000:0.9886000156402588, time is 2427\n",
      "accuracy of test at step 13500:0.9901999831199646, time is 2519\n",
      "accuracy of test at step 14000:0.9894999861717224, time is 2612\n",
      "accuracy of test at step 14500:0.989799976348877, time is 2705\n",
      "accuracy of test at step 15000:0.9898999929428101, time is 2799\n",
      "accuracy of test at step 15500:0.9904000163078308, time is 2892\n",
      "accuracy of test at step 16000:0.9897000193595886, time is 2984\n",
      "accuracy of test at step 16500:0.989799976348877, time is 3084\n",
      "accuracy of test at step 17000:0.989799976348877, time is 3183\n",
      "accuracy of test at step 17500:0.9897000193595886, time is 3282\n",
      "accuracy of test at step 18000:0.9896000027656555, time is 3380\n",
      "accuracy of test at step 18500:0.9902999997138977, time is 3479\n",
      "accuracy of test at step 19000:0.9901999831199646, time is 3579\n",
      "accuracy of test at step 19500:0.9901000261306763, time is 3675\n",
      "accuracy of test at step 20000:0.9897000193595886, time is 3770\n",
      "teain is over, totle time 3770\n"
     ]
    }
   ],
   "source": [
    "mni = Mnist_CNN(f1_ck=FLAGS.f1_ck, f1_cc=FLAGS.f1_cc, f2_ck=FLAGS.f2_ck, \n",
    "                f2_cc=FLAGS.f2_cc, features=FLAGS.features, drop=FLAGS.dropout, \n",
    "                l2_regularizer= FLAGS.l2_regularizer, lr_start=FLAGS.l_r_s, lr_end = FLAGS.l_r_e, \n",
    "                maxstep=FLAGS.iterations, batch=FLAGS.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 结果分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "运行结果保存在tensorboard中，对应文件在同目录下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
